name: Hourly Data Pipeline

# --- Triggers ---
on:
  # 1. Run automatically on a schedule (at the top of every hour)
  schedule:
    - cron: '0 * * * *'

  # 2. Allow you to run this workflow manually from the GitHub Actions tab
  workflow_dispatch:

# --- Job Definition ---
jobs:
  run-pipeline:
    # Use a standard Ubuntu virtual machine
    runs-on: ubuntu-latest

    steps:
      # Step 1: Check out your repository's code
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Set up the Python environment
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # Step 3: Install dependencies and run the pipeline
      - name: Install dependencies and run pipeline
        env:
          PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
        run: |
          # Navigate into the data-pipeline directory
          cd data-pipeline

          # Set up and activate the virtual environment
          python -m venv venv
          source venv/bin/activate

          # Install all required packages
          pip install -r requirements.txt

          # Now, run the master script
          ./run_pipeline.sh
